{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299d3145",
   "metadata": {},
   "source": [
    "Topic : Perform bag-of-words approach (count occurrence, normalized count occurrence), TF-IDF on\n",
    "data. Create embeddings using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464d2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"I want to be a Product Manager\",\n",
    "    \"I love to solve user problems\",\n",
    "    \"Managing product is fun and exciting\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ec2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12980c",
   "metadata": {},
   "source": [
    "Bag Of Words - Count Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc9f1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and' 'be' 'exciting' 'fun' 'is' 'love' 'manager' 'managing' 'problems'\n",
      " 'product' 'solve' 'to' 'user' 'want']\n",
      "Bag of Words (Count):\n",
      " [[0 1 0 0 0 0 1 0 0 1 0 1 0 1]\n",
      " [0 0 0 0 0 1 0 0 1 0 1 1 1 0]\n",
      " [1 0 1 1 1 0 0 1 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "bow_counts = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\", count_vectorizer.get_feature_names_out())\n",
    "print(\"Bag of Words (Count):\\n\", bow_counts.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6dcc9e",
   "metadata": {},
   "source": [
    "Bag Of Words - Normalized Count Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c00cb6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Term Frequency:\n",
      " [[0.         0.2        0.         0.         0.         0.\n",
      "  0.2        0.         0.         0.2        0.         0.2\n",
      "  0.         0.2       ]\n",
      " [0.         0.         0.         0.         0.         0.2\n",
      "  0.         0.         0.2        0.         0.2        0.2\n",
      "  0.2        0.        ]\n",
      " [0.16666667 0.         0.16666667 0.16666667 0.16666667 0.\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = TfidfVectorizer(use_idf=False, norm='l1')\n",
    "tf_matrix = tf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Normalized Term Frequency:\\n\", tf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46845395",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency â€“ Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757148da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and' 'be' 'exciting' 'fun' 'is' 'love' 'manager' 'managing' 'problems'\n",
      " 'product' 'solve' 'to' 'user' 'want']\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.49047908 0.         0.         0.         0.\n",
      "  0.49047908 0.         0.         0.37302199 0.         0.37302199\n",
      "  0.         0.49047908]\n",
      " [0.         0.         0.         0.         0.         0.46735098\n",
      "  0.         0.         0.46735098 0.         0.46735098 0.35543247\n",
      "  0.46735098 0.        ]\n",
      " [0.42339448 0.         0.42339448 0.42339448 0.42339448 0.\n",
      "  0.         0.42339448 0.         0.32200242 0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf10b6",
   "metadata": {},
   "source": [
    "Word Embeddings using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93ad741c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\mansi\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\mansi\\anaconda3\\lib\\site-packages (from gensim) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\mansi\\anaconda3\\lib\\site-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\mansi\\anaconda3\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\mansi\\anaconda3\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n",
      "Vector for 'product':\n",
      " [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_docs,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "# Get vector for a word\n",
    "print(\"Vector for 'product':\\n\", word2vec_model.wv['product'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a8a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
