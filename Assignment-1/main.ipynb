{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1f8751",
   "metadata": {},
   "source": [
    "Assignment-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d62f89",
   "metadata": {},
   "source": [
    "Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK \n",
    "library. Use porter stemmer and snowball stemmer for stemming. Use any technique for \n",
    "lemmatization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2670646e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\mansi\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\mansi\\anaconda3\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\mansi\\anaconda3\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mansi\\anaconda3\\lib\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mansi\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mansi\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6015e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer, TweetTokenizer, MWETokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4acc1a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MANSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MANSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\MANSI\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29eaefe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Natural language processing (NLP) is a subfield of artificial intelligence (AI) focused on the interaction between computers and humans through natural language. The ultimate objective of NLP is to enable computers to understand, interpret, and generate human language in a way that is valuable.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1154d252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Natural language processing (NLP) is a subfield of artificial intelligence (AI) focused on the interaction between computers and humans through natural language. The ultimate objective of NLP is to enable computers to understand, interpret, and generate human language in a way that is valuable.\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c36864e",
   "metadata": {},
   "source": [
    "Tokenization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665514cd",
   "metadata": {},
   "source": [
    "a. Whitespace tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4618a272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whitespace Tokenization:\n",
      "['Natural', 'language', 'processing', '(NLP)', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '(AI)', 'focused', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language.', 'The', 'ultimate', 'objective', 'of', 'NLP', 'is', 'to', 'enable', 'computers', 'to', 'understand,', 'interpret,', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'valuable.']\n",
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "whitespace_tokenized = text.split()\n",
    "print(\"\\nWhitespace Tokenization:\")\n",
    "print(whitespace_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb810d4",
   "metadata": {},
   "source": [
    "b. Punctuation-based tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "527bf7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Punctuation-based Tokenization:\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'focused', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.', 'The', 'ultimate', 'objective', 'of', 'NLP', 'is', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'valuable', '.']\n"
     ]
    }
   ],
   "source": [
    "punctuation_tokenized = word_tokenize(text)\n",
    "print(\"\\nPunctuation-based Tokenization:\")\n",
    "print(punctuation_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "000ef005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Punctuation-Based Tokenization:\n",
      "['Natural', 'language', 'processing', 'NLP', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'AI', 'focused', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', 'The', 'ultimate', 'objective', 'of', 'NLP', 'is', 'to', 'enable', 'computers', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'valuable']\n"
     ]
    }
   ],
   "source": [
    "punctuation_tokens = [word.strip(string.punctuation) for word in whitespace_tokenized if word.strip(string.punctuation)]\n",
    "print(\"\\nPunctuation-Based Tokenization:\")\n",
    "print(punctuation_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea9066b",
   "metadata": {},
   "source": [
    "c. Treebank tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe4379ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treebank Tokenization:\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'focused', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language.', 'The', 'ultimate', 'objective', 'of', 'NLP', 'is', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'valuable', '.']\n"
     ]
    }
   ],
   "source": [
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
    "print(\"\\nTreebank Tokenization:\")\n",
    "print(treebank_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafb2176",
   "metadata": {},
   "source": [
    "d.Tweet tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d6b5240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweet Tokenization:\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'focused', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.', 'The', 'ultimate', 'objective', 'of', 'NLP', 'is', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'valuable', '.']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
    "print(\"\\nTweet Tokenization:\")\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb053b",
   "metadata": {},
   "source": [
    "(e) Multi-Word Expression (MWE) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13051317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-Word Expression (MWE) Tokenization:\n",
      "['I', 'am', 'studying', 'natural_language', 'processing', 'and', 'artificial_intelligence']\n"
     ]
    }
   ],
   "source": [
    "mwe_tokenizer = MWETokenizer([('natural', 'language'), ('artificial', 'intelligence')])\n",
    "mwe_text = \"I am studying natural language processing and artificial intelligence\"\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(mwe_text))\n",
    "\n",
    "print(\"\\nMulti-Word Expression (MWE) Tokenization:\")\n",
    "print(mwe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc7e72c",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49afe26f",
   "metadata": {},
   "source": [
    "a. Porter-Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c91d1120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Porter Stemmer Results:\n",
      "['natur', 'languag', 'process', 'nlp', 'is', 'a', 'subfield', 'of', 'artifici', 'intellig', 'ai', 'focus', 'on', 'the', 'interact', 'between', 'comput', 'and', 'human', 'through', 'natur', 'languag', 'the', 'ultim', 'object', 'of', 'nlp', 'is', 'to', 'enabl', 'comput', 'to', 'understand', 'interpret', 'and', 'gener', 'human', 'languag', 'in', 'a', 'way', 'that', 'is', 'valuabl']\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "porter_stems = [porter.stem(word) for word in punctuation_tokens]\n",
    "print(\"\\nPorter Stemmer Results:\")\n",
    "print(porter_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef58d9a",
   "metadata": {},
   "source": [
    "b. Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "491c2fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Snowball Stemmer Results:\n",
      "['natur', 'languag', 'process', 'nlp', 'is', 'a', 'subfield', 'of', 'artifici', 'intellig', 'ai', 'focus', 'on', 'the', 'interact', 'between', 'comput', 'and', 'human', 'through', 'natur', 'languag', 'the', 'ultim', 'object', 'of', 'nlp', 'is', 'to', 'enabl', 'comput', 'to', 'understand', 'interpret', 'and', 'generat', 'human', 'languag', 'in', 'a', 'way', 'that', 'is', 'valuabl']\n"
     ]
    }
   ],
   "source": [
    "snowball = SnowballStemmer(\"english\")\n",
    "snowball_stems = [snowball.stem(word) for word in punctuation_tokens]\n",
    "print(\"\\nSnowball Stemmer Results:\")\n",
    "print(snowball_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a009657c",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6eecc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization Results:\n",
      "['Natural', 'language', 'processing', 'NLP', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'AI', 'focused', 'on', 'the', 'interaction', 'between', 'computer', 'and', 'human', 'through', 'natural', 'language', 'The', 'ultimate', 'objective', 'of', 'NLP', 'is', 'to', 'enable', 'computer', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'valuable']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in punctuation_tokens]\n",
    "print(\"\\nLemmatization Results:\")\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7fb697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
